{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f5f2a9-9f91-4c48-a49d-4f83a5b3177f",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9c61ec-58e1-421f-af59-9586f6ce77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 10:57:57.604178: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 10:57:57.664245: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-23 10:57:57.664287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-23 10:57:57.666638: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-23 10:57:57.674057: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 10:57:57.674662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 10:57:59.158996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory as image_dataset \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e69867-8b78-4ed0-89b2-b9433ceae00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "      <th>datetimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>48.9</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>299706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1292.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>47.7</td>\n",
       "      <td>0.7255</td>\n",
       "      <td>299707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1402.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.7502</td>\n",
       "      <td>299708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1376.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.7867</td>\n",
       "      <td>299709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1272.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>0.7888</td>\n",
       "      <td>299710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8440</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>1729.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.7568</td>\n",
       "      <td>309058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8441</th>\n",
       "      <td>1163.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>309059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8442</th>\n",
       "      <td>1142.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.6406</td>\n",
       "      <td>309060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5139</td>\n",
       "      <td>309061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8444</th>\n",
       "      <td>1071.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>309062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8445 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PT08.S1(CO)  PT08.S2(NMHC)  PT08.S3(NOx)  PT08.S4(NO2)  PT08.S5(O3)  \\\n",
       "0          1360.0         1046.0        1056.0        1692.0       1268.0   \n",
       "1          1292.0          955.0        1174.0        1559.0        972.0   \n",
       "2          1402.0          939.0        1140.0        1555.0       1074.0   \n",
       "3          1376.0          948.0        1092.0        1584.0       1203.0   \n",
       "4          1272.0          836.0        1205.0        1490.0       1110.0   \n",
       "...           ...            ...           ...           ...          ...   \n",
       "8440       1314.0         1101.0         539.0        1374.0       1729.0   \n",
       "8441       1163.0         1027.0         604.0        1264.0       1269.0   \n",
       "8442       1142.0         1063.0         603.0        1241.0       1092.0   \n",
       "8443       1003.0          961.0         702.0        1041.0        770.0   \n",
       "8444       1071.0         1047.0         654.0        1129.0        816.0   \n",
       "\n",
       "         T    RH      AH  datetimestamp  \n",
       "0     13.6  48.9  0.7578         299706  \n",
       "1     13.3  47.7  0.7255         299707  \n",
       "2     11.9  54.0  0.7502         299708  \n",
       "3     11.0  60.0  0.7867         299709  \n",
       "4     11.2  59.6  0.7888         299710  \n",
       "...    ...   ...     ...            ...  \n",
       "8440  21.9  29.3  0.7568         309058  \n",
       "8441  24.3  23.7  0.7119         309059  \n",
       "8442  26.9  18.3  0.6406         309060  \n",
       "8443  28.3  13.5  0.5139         309061  \n",
       "8444  28.5  13.1  0.5028         309062  \n",
       "\n",
       "[8445 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/set.csv').drop(columns=['DateTime'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b143deb-a9b0-400e-a32c-9c25959cd80f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Results method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefb78ee-41d1-423e-a36d-2cae02c6c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'mse', 'mae', 'rmse', 'mape', 'r2'])\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def appendError(model, testy, y_pred):\n",
    "    model_name = str(type(model).__name__)\n",
    "    mse = mean_squared_error(testy, y_pred)\n",
    "    mae = mean_absolute_error(testy, y_pred)\n",
    "    rmse = root_mean_squared_error(testy, y_pred)\n",
    "    mape = mean_absolute_percentage_error(testy, y_pred)\n",
    "    r2 = r2_score(testy, y_pred)\n",
    "    return pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'mse': mse, \n",
    "        'mae': mae, \n",
    "        'mae': mae, \n",
    "        'rmse': rmse, \n",
    "        'mape': mape, \n",
    "        'r2': r2\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81140205-0e0c-4e8e-92c7-df43816b19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(ds: tf.data.Dataset, df: pd.DataFrame):\n",
    "    print(df.iloc[0])\n",
    "    for images in ds.take(1):\n",
    "        # Plot the first image from the batch\n",
    "        plt.figure()\n",
    "        plt.imshow(images[0].numpy())  # Convert to numpy array and display as image\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d369286-fb08-413e-99e3-9ac627581cc1",
   "metadata": {},
   "source": [
    "# Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2aad474b-e4bf-4102-809d-9c50a00f2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 32\n",
    "image_size = 224\n",
    "data_dir = pathlib.Path('./imagesTML/textINimage/CO/').with_suffix('')\n",
    "data_dir.glob\n",
    "shuffle = False\n",
    "seed = 123123\n",
    "split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75d488ee-58cd-4deb-b987-deb5e3fe2a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1360., 1292., 1402., ..., 1142., 1003., 1071.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2c8f1eea-648e-4dcc-b843-07be9c457535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.float64, name=None)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns=['PT08.S1(CO)'])\n",
    "y = data['PT08.S1(CO)']\n",
    "y_ds = tf.data.Dataset.from_tensor_slices(y)\n",
    "y_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0c5c9be-327d-40fe-a826-8294cf88a595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8445 files belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_ds = image_dataset(\n",
    "    data_dir,\n",
    "    labels=None,\n",
    "    image_size=(image_size, image_size),\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False\n",
    ")\n",
    "whole_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4bdce90-2f1b-41bb-a627-d121e4bacb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ZipDataset element_spec=(TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.zip(whole_ds, y_ds)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cd3932d-540d-48a8-8970-858dcdae11dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "print(len(list(dataset)))\n",
    "print(len(list(whole_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9197b8aa-8d0d-4b3c-9a45-78a5c87824b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 211\n",
      "Validation dataset size: 26\n",
      "Test dataset size: 27\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_ds = dataset.take(train_size)\n",
    "val_ds = dataset.skip(train_size).take(val_size)\n",
    "test_ds = dataset.skip(train_size + val_size)\n",
    "\n",
    "print(f\"Train dataset size: {len(list(train_ds))}\")\n",
    "print(f\"Validation dataset size: {len(list(val_ds))}\")\n",
    "print(f\"Test dataset size: {len(list(test_ds))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c7637-af80-468c-8b35-fcd4b8e0a5eb",
   "metadata": {},
   "source": [
    "# Convoluting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79636036-af35-4583-aa45-ab8818e98003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(image_size, image_size, 1)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4202e0fd-5797-4531-89e6-cc66e8e387ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.mean_squared_error,\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e47fcb34-d03c-4827-bbb1-d6ccc7240c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 291, in __call__\n        batch_dim = tf.shape(y_t)[0]\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filea8vujpkf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/home/miiky/progar/python/chamba/.venv/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 291, in __call__\n        batch_dim = tf.shape(y_t)[0]\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a0f09-7c3f-4222-8c6e-7e3d4e5bb82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
